{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.data import find\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.metrics import *\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Figure, Layout\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    The paraphrase data that will be analysed\n",
    "'''\n",
    "train_input = text_to_sentences('IHLT-eval-framework/train/msr_paraphrase_train_input.txt')\n",
    "test_input = text_to_sentences('IHLT-eval-framework/test/msr_paraphrase_test_input.txt')\n",
    "train_classes = open('IHLT-eval-framework/train/msr_paraphrase_train_gs.txt', encoding=\"utf-8-sig\").readlines()\n",
    "test_classes = open('IHLT-eval-framework/test/msr_paraphrase_test_gs.txt', encoding=\"UTF8\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    A collection of useful functions for nlp\n",
    "'''\n",
    "# Removing the punctuation and lowering the case of a string\n",
    "def remove_punctuation(line):\n",
    "    return line.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# extract the words from the sentence\n",
    "def words_from_sent(sent):\n",
    "    # tokenized\n",
    "    tokenized = word_tokenize(sent)\n",
    "    # remove stopwords and return\n",
    "    return [word for word in tokenized if word not in stopwords.words('english')]\n",
    "\n",
    "# convert words to tokens\n",
    "def tokens_from_words(words):\n",
    "    return pos_tag(words)\n",
    "\n",
    "# Function to get wordnet pos code\n",
    "def wordnet_pos_code(tag):\n",
    "    if tag.startswith('NN'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('VB'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('JJ'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('RB'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "# Tokens to lemmas using wordnet lemmatizer    \n",
    "def tokens_to_lemmas(tokens):\n",
    "    return list(map(token_to_lemmas, tokens))\n",
    "\n",
    "def token_to_lemmas(token):    \n",
    "    pos = wordnet_pos_code(token[1])\n",
    "    if pos:\n",
    "        return WordNetLemmatizer().lemmatize(token[0], pos=pos)\n",
    "    return token[0]\n",
    "\n",
    "# Convert tokens to senses where sense is the first synset + POS\n",
    "def tokens_to_senses(tokens):\n",
    "    return list(filter(None, map(token_to_sense, tokens)))\n",
    "\n",
    "def token_to_sense(token):\n",
    "    lemma = WordNetLemmatizer().lemmatize(token[0])\n",
    "    pos = wordnet_pos_code(token[1])\n",
    "    synsets = []\n",
    "    if pos: \n",
    "        return wordnet.synsets(lemma, pos=pos)\n",
    "\n",
    "# Load the lines of training text as sentences\n",
    "def text_to_sentences(filename):\n",
    "    sentence_pair_array = []\n",
    "    for line in open(filename, encoding=\"UTF8\").readlines():\n",
    "        sentence_pair_array.append([s.strip() for s in line.split(\"\\t\")])\n",
    "    return sentence_pair_array\n",
    "\n",
    "def parse_pos(sentence):\n",
    "    return bllip.parse_one(sentence)\n",
    "\n",
    "def compare_synsets(synset_a, synset_b):\n",
    "    lcs = synset_a.lowest_common_hypernyms(synset_b)\n",
    "    similarity = synset_a.path_similarity(synset_b)\n",
    "    wup_similarity = synset_a.wup_similarity(synset_b)\n",
    "    lin_similarity = synset_a.lin_similarity(synset_b, brown_ic)                        \n",
    "    lch_similarity = synset_a.lch_similarity(synset_b)\n",
    "    return lcs, similarity, wup_similarity, lin_similarity, lch_similarity\n",
    "\n",
    "def count(g,s):\n",
    "    TP = TN = FP = FN = 0\n",
    "    for i in range(0,len(g)):\n",
    "        if (g[i]==s[i] and s[i]==1): TP+=1\n",
    "        if (g[i]==s[i] and s[i]==0): TN+=1\n",
    "        if (g[i]!=s[i] and s[i]==1): FP+=1\n",
    "        if (g[i]!=s[i] and s[i]==0): FN+=1\n",
    "    return [TP,TN,FP,FN]\n",
    "    \n",
    "def MSRP_eval(gs, sys):\n",
    "    [TP,TN,FP,FN] = count(gs,sys)\n",
    "    acc = (TP+TN)/float(TP+TN+FP+FN) # ACCURACY\n",
    "    reject = TN/float(TN+FP) # precision on negative SPECIFICITY\n",
    "    accept = TP/float(TP+FN) # precision on positive SENSITIVITY\n",
    "    print(\"acc=\",acc,\" reject=\",reject,\" accept=\",accept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Testing\n",
      ".............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Results\n",
      "acc= 0.7055072463768116  reject= 0.5988700564971752  accept= 0.7330415754923414\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    First Paraphrase detector approach, jaccard distance of lemmas    \n",
    "'''\n",
    "def lemma_jaccard(sent_0, sent_1):\n",
    "    print('.', end='')\n",
    "    sent_0, sent_1 = remove_punctuation(sent_0.lower()), remove_punctuation(sent_1.lower())\n",
    "    words_0, words_1 = words_from_sent(sent_0), words_from_sent(sent_1)\n",
    "    tokens_0, tokens_1 = tokens_from_words(words_0), tokens_from_words(words_1)\n",
    "    lemmas_0, lemmas_1 = tokens_to_lemmas(tokens_0), tokens_to_lemmas(tokens_1)\n",
    "    return jaccard_distance(set(lemmas_0), set(lemmas_1))\n",
    "\n",
    "print('Training')\n",
    "X_train = [lemma_jaccard(data[0], data[1]) for data in train_input]\n",
    "y_train = [int(line.strip()) for line in train_classes]\n",
    "print('Testing')\n",
    "X_test = [lemma_jaccard(data[0], data[1])for data in test_input]\n",
    "y_test = [int(line.strip()) for line in test_classes]\n",
    "print('Results')\n",
    "regression = LogisticRegression()\n",
    "regression.fit(np.array(X_train).reshape(-1,1), y_train)\n",
    "prediction = regression.predict(np.array(X_test).reshape(-1,1))\n",
    "MSRP_eval(prediction, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>false_negatives</th>\n",
       "      <th>remaining_input</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   false_negatives  remaining_input\n",
       "1                1                1\n",
       "2                1                1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Analysis of results\n",
    "'''\n",
    "# The specificity (reject) is observed to be lower than the other accuracies.\n",
    "# The incorrect values are now selected for analysis:\n",
    "false_negative_idx = np.where((prediction != y_test) & (prediction == 0))[0]\n",
    "false_negatives = []\n",
    "remaining_input = []\n",
    "for idx, el in enumerate(test_input):\n",
    "    if idx in false_negative_idx:\n",
    "        false_negatives.append(el)\n",
    "    else:\n",
    "        remaining_input.append(el)\n",
    "        \n",
    "pd.DataFrame(data = {\n",
    "    'false_negatives': [1],\n",
    "    'remaining_input': [1]\n",
    "}, index=[1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 10)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

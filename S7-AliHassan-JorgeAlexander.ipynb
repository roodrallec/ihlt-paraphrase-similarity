{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "from nltk import data, word_tokenize, pos_tag, ne_chunk, chunk\n",
    "from nltk.corpus import wordnet, stopwords, treebank\n",
    "from nltk.metrics import *\n",
    "from nltk.tag import *\n",
    "from nltk.tokenize import punkt\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from plotly.graph_objs import Scatter, Figure, Layout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "STANFORD_NER_PATH='./english.all.3class.distsim.crf.ser.gz'\n",
    "STANFORD_JAR_PATH='./stanford-ner.jar'\n",
    "train_500 = treebank.tagged_sents()[:500]\n",
    "train_1000 = treebank.tagged_sents()[:1000]\n",
    "train_1500 = treebank.tagged_sents()[:1500]\n",
    "train_2000 = treebank.tagged_sents()[:2000]\n",
    "train_2500 = treebank.tagged_sents()[:2500]\n",
    "train_3000 = treebank.tagged_sents()[:3000]\n",
    "test_data = treebank.tagged_sents()[3001:]\n",
    "TRAINING_DATA = [train_500, train_1000, train_1500, train_2000, train_2500, train_3000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('In', 'IN'),\n",
       " ('early', 'RB'),\n",
       " ('trading', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('Tokyo', 'NNP'),\n",
       " ('Thursday', 'NNP'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('Nikkei', 'NNP'),\n",
       " ('index', 'NN'),\n",
       " ('fell', 'VBD'),\n",
       " ('63.79', 'CD'),\n",
       " ('points', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('35500.64', 'CD'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Part A) Consider Treebank corpus. Train HMM, TnT, perceptron and CRF models\n",
    "# using the first 500, 1000, 1500, 2000, 2500 and 3000 sentences. Evaluate the\n",
    "# resulting 24 models using sentences from 3001.\n",
    "# Provide a figure with four learning curves, each per model type (X=training set\n",
    "# size; Y=accuracy). Which model would you select? Justify the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_hmm(train, test):\n",
    "    print('Train data size: ', str(len(train)))\n",
    "    print('Test data size: ', str(len(test)))\n",
    "    tagger = HiddenMarkovModelTrainer().train_supervised(train)\n",
    "    return tagger.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_tnt(train, test):\n",
    "    print('Train data size: ', str(len(train)))\n",
    "    print('Test data size: ', str(len(test)))\n",
    "    tagger = TnT()\n",
    "    tagger.train(train)\n",
    "    return tagger.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_crf(train, test):\n",
    "    print('Train data size: ', str(len(train)))\n",
    "    print('Test data size: ', str(len(test)))\n",
    "    tagger = CRFTagger()\n",
    "    tagger.train(train, 'crf_tagger_model')\n",
    "    return tagger.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test_pcp(train, test):\n",
    "    print('Train data size: ', str(len(train)))\n",
    "    print('Test data size: ', str(len(test)))\n",
    "    tagger = PerceptronTagger(load=False)\n",
    "    tagger.train(train)\n",
    "    return tagger.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running hmm\n",
      "Train data size:  500\n",
      "Test data size:  913\n",
      "Train data size:  1000\n",
      "Test data size:  913\n",
      "Train data size:  1500\n",
      "Test data size:  913\n",
      "Train data size:  2000\n",
      "Test data size:  913\n",
      "Train data size:  2500\n",
      "Test data size:  913\n",
      "Train data size:  3000\n",
      "Test data size:  913\n",
      "Running tnt\n",
      "Train data size:  500\n",
      "Test data size:  913\n",
      "Train data size:  1000\n",
      "Test data size:  913\n",
      "Train data size:  1500\n",
      "Test data size:  913\n",
      "Train data size:  2000\n",
      "Test data size:  913\n",
      "Train data size:  2500\n",
      "Test data size:  913\n",
      "Train data size:  3000\n",
      "Test data size:  913\n",
      "Running crf\n",
      "Train data size:  500\n",
      "Test data size:  913\n",
      "Train data size:  1000\n",
      "Test data size:  913\n",
      "Train data size:  1500\n",
      "Test data size:  913\n",
      "Train data size:  2000\n",
      "Test data size:  913\n",
      "Train data size:  2500\n",
      "Test data size:  913\n",
      "Train data size:  3000\n",
      "Test data size:  913\n",
      "Running pcp\n",
      "Train data size:  500\n",
      "Test data size:  913\n",
      "Train data size:  1000\n",
      "Test data size:  913\n",
      "Train data size:  1500\n",
      "Test data size:  913\n",
      "Train data size:  2000\n",
      "Test data size:  913\n",
      "Train data size:  2500\n",
      "Test data size:  913\n",
      "Train data size:  3000\n",
      "Test data size:  913\n"
     ]
    }
   ],
   "source": [
    "print('Running hmm')\n",
    "hmm = [train_test_hmm(train_data, test_data) for train_data in TRAINING_DATA]\n",
    "print('Running tnt')\n",
    "tnt = [train_test_tnt(train_data, test_data) for train_data in TRAINING_DATA]\n",
    "print('Running crf')\n",
    "crf = [train_test_crf(train_data, test_data) for train_data in TRAINING_DATA]\n",
    "print('Running pcp')\n",
    "pcp = [train_test_pcp(train_data, test_data) for train_data in TRAINING_DATA]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hmm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-38a7ddf7617c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'hmm'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tnt'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'crf'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pcp'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpcp\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'hmm' is not defined"
     ]
    }
   ],
   "source": [
    "df = {'hmm': hmm, 'tnt': tnt, 'crf': crf, 'pcp': pcp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:///Users/jnalexander/Projects/IHLT-lab7/temp-plot.html'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_data = []\n",
    "for model in df.keys():\n",
    "    plot_data = np.append(plot_data, [Scatter(\n",
    "        x=['train_500', 'train_1000', 'train_1500', 'train_2000', 'train_2500', 'train_3000'],\n",
    "        y=df[model],\n",
    "        name=model\n",
    "    )])\n",
    "layout = Layout(\n",
    "    xaxis=dict(title=\"Training set\"),\n",
    "    yaxis=dict(title=\"Accuracy\")\n",
    ")\n",
    "fig = Figure(data=list(plot_data), layout=layout)\n",
    "plot(fig)\n",
    "\n",
    "# The plot shows that all the algorithms improve accuracy with larger training sets. \n",
    "# It also shows that the pcp and crf models have the highest accuracy.\n",
    "# Finally, it shows that the hmm has the lowest accuracy, but that it improves significantly as the\n",
    "# training size is increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# B) Read the three first pair of sentences of the training file within the\n",
    "# evaluation framework of the project. Compute their similarities by considering\n",
    "# the following approaches:\n",
    "# I words and Jaccard coefficient (same as in Session 5)\n",
    "# I words plus NEs and Jaccard coefficient\n",
    "# Print the results. Do you think it could be relevant to use NEs to compute the\n",
    "# similarity between two sentences? Justify the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pretrained nltk detector\n",
    "sent_detector = data.load('tokenizers/punkt/english.pickle')\n",
    "stanford_tagger = StanfordNERTagger(STANFORD_NER_PATH, STANFORD_JAR_PATH)\n",
    "\n",
    "def lines_to_sentences(line):\n",
    "    return sent_detector.tokenize(line.strip())\n",
    "\n",
    "def remove_punctuation(line):\n",
    "    return line.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def extract_words(sent):\n",
    "    words = word_tokenize(sent)\n",
    "    return [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "def compare_sentences(sent_0, sent_1):\n",
    "    # 1. Extract words from sentences\n",
    "    line_0, line_1 = remove_punctuation(sent_0), remove_punctuation(sent_1)\n",
    "    words_0, words_1 = extract_words(line_0), extract_words(line_1)    \n",
    "    # 2. Jaccard distance between words\n",
    "    print(\"\\nWord set:\\n\", set(words_0), \"\\nand:\\n\", set(words_1))\n",
    "    word_jaccard = jaccard_distance(set(words_0), set(words_1))\n",
    "    # 3. Get NEs from sentences\n",
    "    nes_0, nes_1 = stanford_tagger.tag(words_0), stanford_tagger.tag(words_1)\n",
    "    # 4. Jaccard distance between nes\n",
    "    print(\"\\nNEs set:\\n\", set(nes_0), \"\\nand:\\n\", set(nes_1))\n",
    "    nes_jaccard = jaccard_distance(set(nes_0), set(nes_1))\n",
    "    # 4. Jaccard distance between nes & words\n",
    "    return word_jaccard, nes_jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing\n",
      " Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence. \n",
      "With\n",
      " Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.\n",
      "\n",
      "Word set:\n",
      " {'deliberately', 'distorting', 'witness', 'brother', 'accused', 'evidence', 'Amrozi', 'called'} \n",
      "and:\n",
      " {'deliberately', 'distorting', 'witness', 'brother', 'Referring', 'accused', 'evidence', 'Amrozi'}\n",
      "\n",
      "NEs set:\n",
      " {('witness', 'O'), ('Amrozi', 'PERSON'), ('evidence', 'O'), ('called', 'O'), ('brother', 'O'), ('deliberately', 'O'), ('accused', 'O'), ('distorting', 'O')} \n",
      "and:\n",
      " {('witness', 'O'), ('Referring', 'O'), ('Amrozi', 'PERSON'), ('evidence', 'O'), ('brother', 'O'), ('deliberately', 'O'), ('accused', 'O'), ('distorting', 'O')}\n",
      "Word_jaccard:  0.2222222222222222\n",
      "Nes_jaccard:  0.2222222222222222\n",
      "\n",
      "Comparing\n",
      " Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $2.5 billion. \n",
      "With\n",
      " Yucaipa bought Dominick's in 1995 for $693 million and sold it to Safeway for $1.8 billion in 1998.\n",
      "\n",
      "Word set:\n",
      " {'Dominicks', 'Yucaipa', 'billion', 'selling', 'chain', 'Safeway', '25', 'owned', '1998'} \n",
      "and:\n",
      " {'Dominicks', 'Yucaipa', 'sold', '18', 'billion', 'Safeway', '693', 'million', 'bought', '1998', '1995'}\n",
      "\n",
      "NEs set:\n",
      " {('selling', 'O'), ('Safeway', 'ORGANIZATION'), ('25', 'O'), ('Dominicks', 'ORGANIZATION'), ('owned', 'O'), ('Yucaipa', 'ORGANIZATION'), ('billion', 'O'), ('1998', 'O'), ('chain', 'O')} \n",
      "and:\n",
      " {('Safeway', 'ORGANIZATION'), ('Yucaipa', 'ORGANIZATION'), ('sold', 'O'), ('18', 'O'), ('bought', 'O'), ('Dominicks', 'O'), ('693', 'O'), ('billion', 'O'), ('1998', 'O'), ('million', 'O'), ('1995', 'O')}\n",
      "Word_jaccard:  0.6666666666666666\n",
      "Nes_jaccard:  0.75\n",
      "\n",
      "Comparing\n",
      " They had published an advertisement on the Internet on June 10, offering the cargo for sale, he added. \n",
      "With\n",
      " On June 10, the ship's owners had published an advertisement on the Internet, offering the explosives for sale.\n",
      "\n",
      "Word set:\n",
      " {'advertisement', 'June', 'sale', 'offering', 'They', 'Internet', 'published', 'added', '10', 'cargo'} \n",
      "and:\n",
      " {'advertisement', 'June', 'sale', 'offering', 'ships', 'On', 'published', 'Internet', 'explosives', '10', 'owners'}\n",
      "\n",
      "NEs set:\n",
      " {('They', 'O'), ('published', 'O'), ('June', 'O'), ('sale', 'O'), ('added', 'O'), ('Internet', 'O'), ('offering', 'O'), ('10', 'O'), ('cargo', 'O'), ('advertisement', 'O')} \n",
      "and:\n",
      " {('ships', 'O'), ('On', 'O'), ('published', 'O'), ('June', 'O'), ('sale', 'O'), ('Internet', 'O'), ('offering', 'O'), ('10', 'O'), ('advertisement', 'O'), ('owners', 'O'), ('explosives', 'O')}\n",
      "Word_jaccard:  0.5\n",
      "Nes_jaccard:  0.5\n"
     ]
    }
   ],
   "source": [
    "lines = [msr_line_1, msr_line_2, msr_line_3]\n",
    "for line in lines:\n",
    "    pair = lines_to_sentences(line)\n",
    "    print(\"\\nComparing\\n\", pair[0], \"\\nWith\\n\", pair[1])\n",
    "    word_jaccard, nes_jaccard = compare_sentences(pair[0], pair[1])\n",
    "    print(\"Word_jaccard: \", word_jaccard)\n",
    "    print(\"Nes_jaccard: \", nes_jaccard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do you think it could be relevant to use NEs to compute the\n",
    "# similarity between two sentences? \n",
    "\n",
    "# The printed results show that alone, the NEs are not enough to distinguish between sentences,\n",
    "# as for example a sentence that references a company named 'APPLE' will have the same 'ORGANIZATION'\n",
    "# entity associated with it as a sentence that references a company named 'MICROSOFT'.\n",
    "# However, when combined with words, it provides a greater measure of similarity, as it allows\n",
    "# for a sentence such as 'I bought an APPLE at APPLE' to be distinguished from 'I bought APPLE at \n",
    "# APPLE' for example.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

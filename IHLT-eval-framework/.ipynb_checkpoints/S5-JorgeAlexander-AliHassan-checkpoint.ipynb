{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import punkt\n",
    "from nltk.metrics import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the first three lines of training text\n",
    "with open('train/msr_paraphrase_train_input.txt', 'r') as f:\n",
    "    line1, line2, line3 = next(f), next(f), next(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITIES\n",
    "# Pretrained nltk detector\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# Lines to sentences\n",
    "def lines_to_sentences(line):\n",
    "    return sent_detector.tokenize(line.strip())\n",
    "# Wordnet lemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "# Function to get wordnet pos code\n",
    "def wordnet_pos_code(tag):\n",
    "    if tag.startswith('NN'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('VB'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('JJ'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('RB'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "# Token to lemmas using wordnet lemmatizer\n",
    "def token_to_lemmas(token):    \n",
    "    pos = wordnet_pos_code(token[1])\n",
    "    if pos:\n",
    "        return wnl.lemmatize(token[0], pos=pos)\n",
    "    return token[0]\n",
    "# Convert token to sense where sense is the first synset + POS\n",
    "def token_to_sense(token):\n",
    "    lemma = wnl.lemmatize(token[0])\n",
    "    pos = wordnet_pos_code(token[1])\n",
    "    synsets = []\n",
    "    if pos: \n",
    "        synsets = wordnet.synsets(lemma, pos=pos)\n",
    "    if len(synsets) > 0:\n",
    "        return str(synsets[0]) + pos\n",
    "# Removing the punctuation and lowering the case of a string\n",
    "def remove_punctuation(line):\n",
    "    return line.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def words_from_sent(sent):\n",
    "    # tokenized\n",
    "    tokenized = nltk.word_tokenize(sent)\n",
    "    # remove stopwords and return\n",
    "    return [word for word in tokenized if word not in stopwords.words('english')]\n",
    "\n",
    "# Comparison function\n",
    "def compare_sentences(sent_0, sent_1):\n",
    "    print('==COMPARING==\\n', sent_0, '\\n==WITH==\\n', sent_1)\n",
    "    # Remove the punctuation and make lower case\n",
    "    sent_0, sent_1 = remove_punctuation(sent_0.lower()), remove_punctuation(sent_1.lower())\n",
    "    # Get words from sentences\n",
    "    words_0, words_1 = words_from_sent(sent_0), words_from_sent(sent_1)\n",
    "    print('==WORDS==\\n', words_0, '\\n', words_1)\n",
    "    # Jaccard distance between words\n",
    "    print('word_jaccard_distance: ', jaccard_distance(set(words_0), set(words_1)))\n",
    "    # Split into tokens\n",
    "    tokens_0, tokens_1 = nltk.pos_tag(words_0), nltk.pos_tag(words_1)\n",
    "    print('==TOKENS==\\n', tokens_0, '\\n', tokens_1)\n",
    "    # Split into lemmas\n",
    "    lemmas_0, lemmas_1 = list(map(token_to_lemmas, tokens_0)), list(map(token_to_lemmas, tokens_1))\n",
    "    print('==LEMMAS==\\n', lemmas_0, '\\n', lemmas_1)\n",
    "    # Jaccard distance between lemmas\n",
    "    print('lemma_jaccard_distance: ', jaccard_distance(set(lemmas_0), set(lemmas_1)))\n",
    "    # Split into senses\n",
    "    senses_0, senses_1 = list(filter(None, map(token_to_sense, tokens_0))), list(filter(None, map(token_to_sense, tokens_1)))\n",
    "    print('==SENSES==\\n', senses_0, '\\n', senses_1)\n",
    "    # Jaccard distance between senses\n",
    "    print('sense_jaccard_distance: ', jaccard_distance(set(senses_0), set(senses_1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==COMPARING==\n",
      " Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence. \n",
      "==WITH==\n",
      " Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.\n",
      "==WORDS==\n",
      " ['amrozi', 'accused', 'brother', 'called', 'witness', 'deliberately', 'distorting', 'evidence'] \n",
      " ['referring', 'witness', 'amrozi', 'accused', 'brother', 'deliberately', 'distorting', 'evidence']\n",
      "word_jaccard_distance:  0.2222222222222222\n",
      "==TOKENS==\n",
      " [('amrozi', 'NN'), ('accused', 'VBD'), ('brother', 'NN'), ('called', 'VBN'), ('witness', 'NN'), ('deliberately', 'RB'), ('distorting', 'VBG'), ('evidence', 'NN')] \n",
      " [('referring', 'VBG'), ('witness', 'NN'), ('amrozi', 'NN'), ('accused', 'VBD'), ('brother', 'RBR'), ('deliberately', 'RB'), ('distorting', 'VBG'), ('evidence', 'NN')]\n",
      "==LEMMAS==\n",
      " ['amrozi', 'accuse', 'brother', 'call', 'witness', 'deliberately', 'distort', 'evidence'] \n",
      " ['refer', 'witness', 'amrozi', 'accuse', 'brother', 'deliberately', 'distort', 'evidence']\n",
      "lemma_jaccard_distance:  0.2222222222222222\n",
      "==SENSES==\n",
      " [\"Synset('accuse.v.01')v\", \"Synset('brother.n.01')n\", \"Synset('name.v.01')v\", \"Synset('witness.n.01')n\", \"Synset('intentionally.r.01')r\", \"Synset('falsify.v.01')v\", \"Synset('evidence.n.01')n\"] \n",
      " [\"Synset('mention.v.01')v\", \"Synset('witness.n.01')n\", \"Synset('accuse.v.01')v\", \"Synset('intentionally.r.01')r\", \"Synset('falsify.v.01')v\", \"Synset('evidence.n.01')n\"]\n",
      "sense_jaccard_distance:  0.375\n"
     ]
    }
   ],
   "source": [
    "# Compare line1 pair1\n",
    "sentences = lines_to_sentences(line1)\n",
    "compare_sentences(sentences[0], sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summary\n",
    "# word_jaccard_distance:  0.2222222222222222\n",
    "# lemma_jaccard_distance: 0.2222222222222222\n",
    "# sense_jaccard_distance: 0.375\n",
    "# \n",
    "# A lower jaccard distance means a greater similarity between the sentences.\n",
    "# This would suggest that the two sentences are more similar in word composition than meaning.\n",
    "# This disagrees with our intuition as we understand the sentences the same. \n",
    "# The reason this has happened is because in sentence two, the lemma 'brother' has a different\n",
    "# POS tag than in sentence one (wordnet.ADV instead of wordnet.NOUN) and no synsets can be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==COMPARING==\n",
      " Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $2.5 billion. \n",
      "==WITH==\n",
      " Yucaipa bought Dominick's in 1995 for $693 million and sold it to Safeway for $1.8 billion in 1998.\n",
      "==WORDS==\n",
      " ['yucaipa', 'owned', 'dominicks', 'selling', 'chain', 'safeway', '1998', '25', 'billion'] \n",
      " ['yucaipa', 'bought', 'dominicks', '1995', '693', 'million', 'sold', 'safeway', '18', 'billion', '1998']\n",
      "word_jaccard_distance:  0.6666666666666666\n",
      "==TOKENS==\n",
      " [('yucaipa', 'NN'), ('owned', 'VBD'), ('dominicks', 'NNS'), ('selling', 'VBG'), ('chain', 'NN'), ('safeway', 'RB'), ('1998', 'CD'), ('25', 'CD'), ('billion', 'CD')] \n",
      " [('yucaipa', 'RB'), ('bought', 'VBD'), ('dominicks', 'NNS'), ('1995', 'CD'), ('693', 'CD'), ('million', 'CD'), ('sold', 'VBN'), ('safeway', 'RB'), ('18', 'CD'), ('billion', 'CD'), ('1998', 'NNS')]\n",
      "==LEMMAS==\n",
      " ['yucaipa', 'own', 'dominick', 'sell', 'chain', 'safeway', '1998', '25', 'billion'] \n",
      " ['yucaipa', 'buy', 'dominick', '1995', '693', 'million', 'sell', 'safeway', '18', 'billion', '1998']\n",
      "lemma_jaccard_distance:  0.5714285714285714\n",
      "==SENSES==\n",
      " [\"Synset('own.v.01')v\", \"Synset('dominique.n.01')n\", \"Synset('sell.v.01')v\", \"Synset('chain.n.01')n\"] \n",
      " [\"Synset('buy.v.01')v\", \"Synset('dominique.n.01')n\", \"Synset('sell.v.01')v\"]\n",
      "sense_jaccard_distance:  0.6\n"
     ]
    }
   ],
   "source": [
    "# Compare line2 pair2\n",
    "sentences = lines_to_sentences(line2)\n",
    "compare_sentences(sentences[0], sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summary\n",
    "# word_jaccard_distance:  0.6666666666666666\n",
    "# lemma_jaccard_distance: 0.5714285714285714\n",
    "# sense_jaccard_distance: 0.6\n",
    "#  \n",
    "# The results suggest that the sentences have roughly an equal similarity in words, lemma and sense.\n",
    "# Also, it would suggest that they are less similar than in pair one, as they have a higher jaccard_distance coefficient.\n",
    "# Intuition agrees with this as in sentence two Safeway is sold for $1.8 billion instead of the $2.5 billion\n",
    "# in sentence one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==COMPARING==\n",
      " They had published an advertisement on the Internet on June 10, offering the cargo for sale, he added. \n",
      "==WITH==\n",
      " On June 10, the ship's owners had published an advertisement on the Internet, offering the explosives for sale.\n",
      "==WORDS==\n",
      " ['published', 'advertisement', 'internet', 'june', '10', 'offering', 'cargo', 'sale', 'added'] \n",
      " ['june', '10', 'ships', 'owners', 'published', 'advertisement', 'internet', 'offering', 'explosives', 'sale']\n",
      "word_jaccard_distance:  0.4166666666666667\n",
      "==TOKENS==\n",
      " [('published', 'VBN'), ('advertisement', 'JJ'), ('internet', 'NN'), ('june', 'NN'), ('10', 'CD'), ('offering', 'NN'), ('cargo', 'NN'), ('sale', 'NN'), ('added', 'VBD')] \n",
      " [('june', 'NN'), ('10', 'CD'), ('ships', 'NNS'), ('owners', 'NNS'), ('published', 'VBN'), ('advertisement', 'JJ'), ('internet', 'NN'), ('offering', 'NN'), ('explosives', 'NNS'), ('sale', 'NN')]\n",
      "==LEMMAS==\n",
      " ['publish', 'advertisement', 'internet', 'june', '10', 'offering', 'cargo', 'sale', 'add'] \n",
      " ['june', '10', 'ship', 'owner', 'publish', 'advertisement', 'internet', 'offering', 'explosive', 'sale']\n",
      "lemma_jaccard_distance:  0.4166666666666667\n",
      "==SENSES==\n",
      " [\"Synset('print.v.01')v\", \"Synset('internet.n.01')n\", \"Synset('june.n.01')n\", \"Synset('offer.n.02')n\", \"Synset('cargo.n.01')n\", \"Synset('sale.n.01')n\", \"Synset('add.v.01')v\"] \n",
      " [\"Synset('june.n.01')n\", \"Synset('ship.n.01')n\", \"Synset('owner.n.01')n\", \"Synset('print.v.01')v\", \"Synset('internet.n.01')n\", \"Synset('offer.n.02')n\", \"Synset('explosive.n.01')n\", \"Synset('sale.n.01')n\"]\n",
      "sense_jaccard_distance:  0.5\n"
     ]
    }
   ],
   "source": [
    "# Compare line3 pair3\n",
    "sentences = lines_to_sentences(line3)\n",
    "compare_sentences(sentences[0], sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summary\n",
    "# word_jaccard_distance:  0.4166666666666667\n",
    "# lemma_jaccard_distance: 0.4166666666666667\n",
    "# sense_jaccard_distance: 0.5\n",
    "# \n",
    "# These results suggest that the word composition is slightly more similar than the sense of the \n",
    "# two sentences. However, this doesn't agree with our human understanding as we understand the\n",
    "# sentences as having the same meaning, just that sentence two is more specific about the cargo\n",
    "# being explosives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Which one of these approaches, if any, do you think that could perform better\n",
    "# for any pair of texts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Given the small nature of the corpus we have used, it's difficult to say which one would perform better\n",
    "# on a larger dataset. However, the results show that the sense_jaccard_distance does not agree\n",
    "# with our human understanding in line3 pair3 even though it was a good measure for similarity\n",
    "# line1 pair1.\n",
    "# The word and lemma similarity measures perform quite similarly, so it may be ok to only use one\n",
    "# of them, the lemma similarity can identify similarities when the sentence has the same meaning\n",
    "# but words placed differently, and therefore may be more useful. \n",
    "# Overall, we think that combining all three measures would be the best approach as it compares\n",
    "# the similarities of sentences across multiple dimensions, perhaps taking an average of the three\n",
    "# or first seeing what distribution they have over larger bodies of text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
